{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pickle import load, dump, dumps\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_pickle('../data/feature_dumps/scaled_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.drop(columns=['NN', 'NNPS', 'VBD', 'VBZ', 'MD',\n",
    "       'EX', 'IN', 'VB', 'JJR', 'JJS', 'PRP', 'WDT', 'JJ', 'VBP', 'NNS',\n",
    "       'VBN', 'DT', 'RB', 'WP', 'VBG', 'NNP', 'RBR', 'PRP$', 'JJ NN',\n",
    "       'VBP VB', 'VBD PRP', 'IN IN', 'NNP VBZ', 'RB DT', 'NN VBG',\n",
    "       'IN JJ', 'NN NN', 'RB VBZ', 'VBG DT', 'NN NNS', 'VBZ JJ', 'IN RB',\n",
    "       'JJ JJ', 'NN VBZ', 'IN VBG', 'VBP DT', 'VB NN', 'NNS VBP',\n",
    "       'DT NNP', 'PRP VBZ', 'PRP VBD', 'PRP VB', 'NN PRP', 'NN DT',\n",
    "       'VBZ VB', 'PRP VBP', 'PRP$ JJ', 'VBD IN', 'VB JJ', 'NN JJ',\n",
    "       'RB VB', 'JJ NNP', 'RB VBG', 'VBZ PRP', 'VBD DT', 'RB RB', 'JJ VB',\n",
    "       'PRP RB', 'JJ IN', 'VBD VB', 'VB IN', 'VBP PRP', 'VBD RB',\n",
    "       'VBG IN', 'PRP IN', 'VB PRP', 'NN RB', 'NNP NN', 'VB VBN',\n",
    "       'NN NNP', 'IN NN', 'VBP VBN', 'NN WDT', 'RB IN', 'DT JJ', 'RB VBD',\n",
    "       'VBZ IN', 'NN MD', 'VB DT', 'NNS IN', 'NNP RB', 'VB PRP$',\n",
    "       'VBP IN', 'RB VBP', 'NNS RB', 'DT NN', 'VBZ VBN', 'MD RB',\n",
    "       'NNP IN', 'NN VBD', 'JJ NNS', 'NN VB', 'IN PRP$', 'MD VB',\n",
    "       'RB PRP', 'NNS VB', 'VBZ DT', 'VBG NN', 'VBN IN', 'PRP$ NNS',\n",
    "       'VB VB', 'VBP RB', 'NNP NNP', 'NN IN', 'VB RB', 'VBG PRP',\n",
    "       'PRP MD', 'IN DT', 'NNP VBD', 'IN NNS', 'IN NNP', 'RB JJ',\n",
    "       'IN PRP', 'VBD JJ', 'RB VBN', 'DT NNS', 'VBD VBN', 'PRP$ NN',\n",
    "       'VBZ RB', 'VBP JJ', 'DT NN VBZ', 'PRP VBD IN', 'JJ NNP NN',\n",
    "       'DT NNP NNP', 'RB IN PRP', 'IN JJ NN', 'JJ NN VB', 'IN NN IN',\n",
    "       'IN DT NNS', 'NNS IN PRP', 'NN IN NNS', 'VBN IN DT', 'PRP VBP JJ',\n",
    "       'DT NNP NN', 'VBZ DT NN', 'PRP VBP RB', 'IN DT JJ', 'MD RB VB',\n",
    "       'JJ NN IN', 'RB DT NN', 'VBG DT NN', 'NNP NN NN', 'DT NNS IN',\n",
    "       'DT NN IN', 'JJ NN NN', 'VB DT NN', 'DT JJ NNS', 'NN IN PRP$',\n",
    "       'IN JJ NNS', 'NNS IN NN', 'VBZ DT JJ', 'PRP VBP DT', 'NNP NNP NN',\n",
    "       'PRP VBP PRP', 'VB DT JJ', 'VB PRP$ NN', 'DT JJ JJ', 'IN PRP VBP',\n",
    "       'DT NN VB', 'IN NNP NNP', 'NNS IN DT', 'VB IN DT', 'IN DT NNP',\n",
    "       'NN IN NNP', 'IN PRP$ NN', 'DT JJ NN', 'NN NN IN', 'IN NN NN',\n",
    "       'VBP DT NN', 'PRP MD VB', 'PRP$ NN NN', 'RB JJ NN', 'DT NN NN',\n",
    "       'IN PRP$ JJ', 'VBD DT JJ', 'PRP VBP VB', 'NN NN NN', 'VBD DT NN',\n",
    "       'NNP NNP NNP', 'PRP VBD VB', 'JJ NNS IN', 'IN DT NN', 'NN IN NN',\n",
    "       'PRP VBP IN', 'PRP VBD DT', 'IN PRP VBD', 'NN IN JJ', 'DT NN RB',\n",
    "       'NN IN PRP', 'NN IN DT', 'RB IN DT', 'JJ JJ NN', 'PRP$ NN IN',\n",
    "       'RB PRP VBP', 'PRP$ JJ NN', 'IN DT JJ NN', 'NN IN DT NN',\n",
    "       'NN IN PRP$ NN', 'NN IN DT JJ', 'IN DT NN VB', 'DT JJ NN IN',\n",
    "       'IN DT NN NN', 'DT JJ NN NN', 'VB DT JJ NN', 'IN DT NNP NN',\n",
    "       'DT NN IN PRP', 'JJ NN IN DT', 'DT NN IN NN', 'NNS IN DT NN',\n",
    "       'VB DT NN IN', 'NN IN JJ NN', 'DT NN IN DT', 'JJ NN IN NN',\n",
    "       'IN DT NN IN', 'DT NN IN DT NN', 'NN IN DT JJ NN',\n",
    "       'DT JJ NN IN NN', 'JJ NN IN DT NN', 'DT VBZ', 'DT RB', 'VBD NN',\n",
    "       'NNP VB', 'IN NNP NN', 'JJ NNS VB', 'PRP$ NN VB', 'VBN IN NN',\n",
    "       'NN NN VB', 'PRP RB VB', 'VBZ NN', 'DT IN', 'DT VB', 'RB NN',\n",
    "       'WP VB', 'VBP NN', 'DT DT', 'VBN NN', 'IN VB', 'WDT VB',\n",
    "       'IN PRP VB', 'NNP NNP VB', 'NN IN VB', 'RB PRP VB', 'NN PRP VB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Blog', 'Gender', 'POS', 'FMeasure', 'CharLength', 'TFPunctuation',\n",
       "       'TFStopWords', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8',\n",
       "       'f9', 'f10', 'f11', 'ConversationCount', 'AtHomeCount',\n",
       "       'FamilyCount', 'TimeCount', 'WorkCount', 'PastActionsCount',\n",
       "       'GamesCount', 'InternetCount', 'LocationCount', 'FunCount',\n",
       "       'Food/ClothesCount', 'PoeticCount', 'Books/MoviesCount',\n",
       "       'ReligionCount', 'RomanceCount', 'SwearingCount', 'PoliticsCount',\n",
       "       'MusicCount', 'SchoolCount', 'BusinessCount', 'PositiveCount',\n",
       "       'NegativeCount', 'EmotionCount', 'ProperNounCount',\n",
       "       'SentenceCount', 'AvgSentLength', 'UpperCaseChars',\n",
       "       'UpperCaseWords', 'TitleCaseWords'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = None\n",
    "cvb = CountVectorizer(max_features=max_features) # CountVectorizer for blogs\n",
    "cvbb = CountVectorizer(max_features=max_features, binary=True) # Binary CountVectorizer for blogs\n",
    "cvp = CountVectorizer(max_features=max_features) # CountVectorizer for POS\n",
    "cvpb = CountVectorizer(max_features=max_features, binary=True) # Binary CountVectorizer for POS\n",
    "cvbn = CountVectorizer(max_features=max_features, ngram_range=(2, 7)) # CountVectorizer for blog ngrams\n",
    "cvbnb = CountVectorizer(max_features=max_features, binary=True, ngram_range=(2, 7)) # Binary CountVectorizer for blog ngrams\n",
    "cvpn = CountVectorizer(max_features=max_features, ngram_range=(2, 7)) # CountVectorizer for POS ngrams\n",
    "cvpnb = CountVectorizer(max_features=max_features, binary=True, ngram_range=(2, 7)) # Binary CountVectorizer for POS ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs = features.Blog.astype(str)\n",
    "pos = features.POS.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvbf = cvb.fit_transform(blogs)\n",
    "cvbbf = cvbb.fit_transform(blogs)\n",
    "# cvbnf = cvbn.fit_transform(blogs)\n",
    "cvbnbf = cvbnb.fit_transform(blogs)\n",
    "# cvpf = cvp.fit_transform(pos)\n",
    "cvpbf = cvpb.fit_transform(pos)\n",
    "# cvpnf = cvpn.fit_transform(pos)\n",
    "cvpnbf = cvpnb.fit_transform(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features = features[['FMeasure', 'CharLength', 'TFPunctuation',\n",
    "       'TFStopWords', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8',\n",
    "       'f9', 'f10', 'f11', 'ConversationCount', 'AtHomeCount',\n",
    "       'FamilyCount', 'TimeCount', 'WorkCount', 'PastActionsCount',\n",
    "       'GamesCount', 'InternetCount', 'LocationCount', 'FunCount',\n",
    "       'Food/ClothesCount', 'PoeticCount', 'Books/MoviesCount',\n",
    "       'ReligionCount', 'RomanceCount', 'SwearingCount', 'PoliticsCount',\n",
    "       'MusicCount', 'SchoolCount', 'BusinessCount', 'PositiveCount',\n",
    "       'NegativeCount', 'EmotionCount', 'ProperNounCount',\n",
    "       'SentenceCount', 'AvgSentLength', 'UpperCaseChars',\n",
    "       'UpperCaseWords', 'TitleCaseWords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fb = hstack((cvbf, other_features), format='csr')\n",
    "# fbb = hstack((cvbbf, other_features), format='csr')\n",
    "# fbn = hstack((cvbnf, other_features), format='csr')\n",
    "# fbnb = hstack((cvbnbf, other_features), format='csr')\n",
    "# fp = hstack((cvpf, other_features), format='csr')\n",
    "# fpb = hstack((cvpbf, other_features), format='csr')\n",
    "# fpn = hstack((cvpnf, other_features), format='csr')\n",
    "# fpnb = hstack((cvpnbf, other_features), format='csr')\n",
    "# fboth = hstack((cvbf, cvpf, other_features), format='csr')\n",
    "fbothb = hstack((cvbbf, cvpbf, other_features), format='csr')\n",
    "# fbothn = hstack((cvbnf, cvpnf, other_features), format='csr')\n",
    "fbothnb = hstack((cvbnbf, cvpnbf, other_features), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [fb, fbb, fbn, fbnb, fp, fpb, fpn, fpnb, fboth, fbothb, fbothn, fbothnb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ctr = 1\n",
    "for feature in feature_list:\n",
    "    print(\"Feature set \", ctr)\n",
    "    svc = SVC(C=5.0, gamma='auto', kernel='linear', max_iter=3000)\n",
    "#     mlp = MLPClassifier(hidden_layer_sizes=(25, 25), solver='adam', learning_rate='constant', early_stopping=True, max_iter=1000)\n",
    "    score = cross_val_score(svc, feature, features.Gender, cv=10, n_jobs=1)\n",
    "    print(\"SVC scores = \", score, \"Mean = \", np.mean(score))\n",
    "#     score = cross_val_score(mlp, feature, features.Gender, cv=10, n_jobs=1)\n",
    "#     print(\"MLP scores = \", score, \"Mean\", np.mean(score))\n",
    "    print()\n",
    "    ctr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/feature_dumps/both_binary_and_scaled.pkl', 'wb') as pkldump:\n",
    "    dump(fbothb, pkldump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import GenericUnivariateSelect, chi2, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3212, 52542)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbothb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3212, 8635759)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbothnb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = GenericUnivariateSelect(chi2, 'k_best', param=50000)\n",
    "fbothbt = transformer.fit_transform(fbothb, features.Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = GenericUnivariateSelect(chi2, 'k_best', param=50000)\n",
    "fbothnbt = transformer.fit_transform(fbothnb, features.Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3212, 50000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbothbt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3212, 50000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbothnbt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=15, gamma='auto', kernel='linear', max_iter=-1)\n",
    "score = cross_val_score(svc, fbothbt, features.Gender, cv=10, n_jobs=1)\n",
    "print(\"SVC scores = \", score, \"Mean = \", np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(75, 75), solver='adam', learning_rate='constant', early_stopping=True, max_iter=2500, activation='identity')\n",
    "score = cross_val_score(mlp, fbothbt, features.Gender, cv=10, n_jobs=1)\n",
    "print(\"MLP scores = \", score, \"Mean = \", np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP scores =  [0.9752322  0.96884735 0.91588785 0.91588785 0.92523364 0.9376947\n",
      " 0.97507788 0.95638629 0.94080997 0.93457944] Mean =  0.9445637182566091\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(75, 75), solver='adam', learning_rate='constant', early_stopping=True, max_iter=2500, activation='identity')\n",
    "score = cross_val_score(mlp, fbothnbt, features.Gender, cv=10, n_jobs=1)\n",
    "print(\"MLP scores = \", score, \"Mean = \", np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "with open('../data/feature_dumps/word_pos_binary_ngrams_50k_best', 'wb') as pkldump:\n",
    "    dump(fbothnbt, pkldump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump as jdump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdump(mlp, '../models/mlp_v7_unreal', compress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_cold = BaggingClassifier(mlp, n_jobs=1)\n",
    "bag_warm = BaggingClassifier(mlp, warm_start=True, n_jobs=1)\n",
    "extra_cold = ExtraTreesClassifier(mlp, n_jobs=1)\n",
    "extra_warm = ExtraTreesClassifier(mlp, warm_start=True, n_jobs=1)\n",
    "random_cold = RandomForestClassifier(mlp, n_jobs=1)\n",
    "random_warm = RandomForestClassifier(mlp, warm_start=True, n_jobs=1)\n",
    "ada = AdaBoostClassifier(mlp)\n",
    "gbm = GradientBoostingClassifier(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = cross_val_score(bag_cold, fbothbt, features.Gender, cv=10, n_jobs=1)\n",
    "# print(\"Bagging cold start scores = \", score, \"Mean = \", np.mean(score))\n",
    "# score = cross_val_score(bag_warm, fbothbt, features.Gender, cv=10, n_jobs=1)\n",
    "# print(\"Bagging Warm start scores = \", score, \"Mean = \", np.mean(score))\n",
    "# score = cross_val_score(extra_cold, fbothbt, features.Gender, cv=10, n_jobs=1)\n",
    "# print(\"ExtraTrees cold start scores = \", score, \"Mean = \", np.mean(score))\n",
    "# score = cross_val_score(extra_warm, fbothbt, features.Gender, cv=10, n_jobs=1)\n",
    "# print(\"ExtraTrees warm start scores = \", score, \"Mean = \", np.mean(score))\n",
    "# score = cross_val_score(random_cold, fbothbt, features.Gender, cv=10, n_jobs=1)\n",
    "# print(\"RandomForest cold start scores = \", score, \"Mean = \", np.mean(score))\n",
    "# score = cross_val_score(random_warm, fbothbt, features.Gender, cv=10, n_jobs=1)\n",
    "# print(\"RadnomForest warm start scores = \", score, \"Mean = \", np.mean(score))\n",
    "score = cross_val_score(ada, fbothbt, features.Gender, cv=10, n_jobs=1)\n",
    "print(\"Adaboost scores = \", score, \"Mean = \", np.mean(score))\n",
    "score = cross_val_score(gbm, fbothbt, features.Gender, cv=10, n_jobs=1)\n",
    "print(\"GradientBoosting scores = \", score, \"Mean = \", np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "n_layers = list(range(45, 85, 10))\n",
    "n_units = list(range(45, 85, 10))\n",
    "depth = list(product(n_layers, n_units))\n",
    "activations = ['relu', 'logistic', 'tanh', 'identity']\n",
    "solvers = ['adam']\n",
    "learning_rates = ['constant']\n",
    "max_iters = [400, 1000, 1600]\n",
    "early_stopping = [True]\n",
    "params_grid = {\n",
    "    'hidden_layer_sizes': depth,\n",
    "    'activation': activations,\n",
    "    'solver': solvers,\n",
    "    'learning_rate': learning_rates,\n",
    "    'max_iter': max_iters,\n",
    "    'early_stopping': early_stopping\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(MLPClassifier(), params_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_result = grid_search.fit(fbothbt, features.Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_result.best_params_)\n",
    "print(grid_result.best_estimator_)\n",
    "print(grid_result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "btr, bte, gtr, gte = train_test_split(fbothnbt, features.Gender, test_size=0.1, stratify=features.Gender, shuffle=True)\n",
    "btr2, bv, gtr2, gv = train_test_split(btr, gtr, test_size=0.18, stratify=gtr, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9078694817658349\n",
      "0.9565217391304348\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(75, 75), activation='identity', early_stopping=True, solver='adam', \n",
    "                    max_iter=2500, learning_rate='constant')\n",
    "mlp.fit(btr2, gtr2)\n",
    "print(mlp.score(bv, gv))\n",
    "print(mlp.score(bte, gte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score =  [0.96551724 0.93103448 0.93103448 0.96539792 0.92733564 0.91695502\n",
      " 0.93771626 0.94791667 0.94444444 0.95833333]\n",
      "Average score =  0.9425685495631655\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(mlp, btr, gtr, cv=10, n_jobs=1)\n",
    "print(\"Score = \", score)\n",
    "print(\"Average score = \", np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdump(mlp, '../models/final/mlp_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/feature_dumps/final/word_pos_ngram_binary_final.pkl', 'wb') as pkldump:\n",
    "    dump(fbothnbt, pkldump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
