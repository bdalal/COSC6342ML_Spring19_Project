{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Clustering text documents using k-means\n",
    "\n",
    "\n",
    "This is an example showing how the scikit-learn can be used to cluster\n",
    "documents by topics using a bag-of-words approach. This example uses\n",
    "a scipy.sparse matrix to store the features instead of standard numpy arrays.\n",
    "\n",
    "Two feature extraction methods can be used in this example:\n",
    "\n",
    "  - TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most\n",
    "    frequent words to features indices and hence compute a word occurrence\n",
    "    frequency (sparse) matrix. The word frequencies are then reweighted using\n",
    "    the Inverse Document Frequency (IDF) vector collected feature-wise over\n",
    "    the corpus.\n",
    "\n",
    "  - HashingVectorizer hashes word occurrences to a fixed dimensional space,\n",
    "    possibly with collisions. The word count vectors are then normalized to\n",
    "    each have l2-norm equal to one (projected to the euclidean unit-ball) which\n",
    "    seems to be important for k-means to work in high dimensional space.\n",
    "\n",
    "    HashingVectorizer does not provide IDF weighting as this is a stateless\n",
    "    model (the fit method does nothing). When IDF weighting is needed it can\n",
    "    be added by pipelining its output to a TfidfTransformer instance.\n",
    "\n",
    "Two algorithms are demoed: ordinary k-means and its more scalable cousin\n",
    "minibatch k-means.\n",
    "\n",
    "Additionally, latent semantic analysis can also be used to reduce dimensionality\n",
    "and discover latent patterns in the data. \n",
    "\n",
    "It can be noted that k-means (and minibatch k-means) are very sensitive to\n",
    "feature scaling and that in this case the IDF weighting helps improve the\n",
    "quality of the clustering by quite a lot as measured against the \"ground truth\"\n",
    "provided by the class label assignments of the 20 newsgroups dataset.\n",
    "\n",
    "This improvement is not visible in the Silhouette Coefficient which is small\n",
    "for both as this measure seem to suffer from the phenomenon called\n",
    "\"Concentration of Measure\" or \"Curse of Dimensionality\" for high dimensional\n",
    "datasets such as text data. Other measures such as V-measure and Adjusted Rand\n",
    "Index are information theoretic based evaluation scores: as they are only based\n",
    "on cluster assignments rather than distances, hence not affected by the curse\n",
    "of dimensionality.\n",
    "\n",
    "Note: as k-means is optimizing a non-convex objective function, it will likely\n",
    "end up in a local optimum. Several runs with independent random init might be\n",
    "necessary to get a good convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Lars Buitinck\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some categories from the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "done in 1.452787s\n",
      "\n",
      "n_samples: 1671, n_features: 17308\n",
      "\n",
      "n_samples: 1541, n_features: 15361\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('../data/feature_dumps/features.pkl')\n",
    "blogsM = df[df.Gender == 'M'].Blog.values.astype(str)\n",
    "blogsF = df[df.Gender == 'F'].Blog.values.astype(str)\n",
    "\n",
    "n_features = 20000\n",
    "\n",
    "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "\n",
    "vectorizerM = TfidfVectorizer(max_df=0.5, max_features=n_features,\n",
    "                         min_df=2, stop_words='english',\n",
    "                         use_idf=True)\n",
    "vectorizerF = TfidfVectorizer(max_df=0.5, max_features=n_features,\n",
    "                         min_df=2, stop_words='english',\n",
    "                         use_idf=True)\n",
    "\n",
    "XM = vectorizerM.fit_transform(blogsM)\n",
    "XF = vectorizerF.fit_transform(blogsF)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"\\nn_samples: %d, n_features: %d\" % XM.shape)\n",
    "print(\"\\nn_samples: %d, n_features: %d\" % XF.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the actual clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 7.232s\n",
      "\n",
      "Top terms per cluster:\n",
      "\n",
      "Males:\n",
      "KMeans:\n",
      "Cluster 0: em youtube band saxophone nodes just prototyping video tools mp3\n",
      "Cluster 1: like food wid days ready wud im person best mr\n",
      "Cluster 2: just ice time week home day work hard sick told\n",
      "Cluster 3: data don paper like kosovo think conference use serbia model\n",
      "Cluster 4: god faith does sin things christ hide heaven bless jesus\n",
      "Cluster 5: love heart know just like world don man think want\n",
      "Cluster 6: person gud college tht talk nakkals guy pain life fan\n",
      "Cluster 7: friend guy best good lol ma gud person class frnd\n",
      "Cluster 8: blog post new com site cancer video ve twitter ll\n",
      "Cluster 9: like new just time year day good did got people\n",
      "Cluster 10: nbsp just lake people blog 2009 like python world skywatch\n",
      "Cluster 11: game games play metroid team wii mario time tournament just\n",
      "Cluster 12: relationship partner feel problems time really need life csi end\n",
      "Cluster 13: women men fat muscle skin excess burn higher woman mass\n",
      "Cluster 14: google mobile web microsoft iphone internet social services phone windows\n",
      "Cluster 15: people life like time just know things think don really\n",
      "Cluster 16: school teacher students education research college class people high make\n",
      "Cluster 17: health obama care senate president house reform democrats vote republicans\n",
      "Cluster 18: abt person ur dont know tat orkut brother lik really\n",
      "Cluster 19: separation divorce formal marriage working legal spouse stress time informal\n",
      "\n",
      "MiniBatch:\n",
      "Cluster 0: hindi leroy laurie topps cards huts thanks hut monks tidy\n",
      "Cluster 1: god life things relationship just time feel job know like\n",
      "Cluster 2: new google web people use twitter like book site mobile\n",
      "Cluster 3: dad told dinner presents dr nakkals vegetarian menu rahul time\n",
      "Cluster 4: time life people like really just friends school good know\n",
      "Cluster 5: best song friend sticker album stickers music guy myspace just\n",
      "Cluster 6: like time just good people know new ve don day\n",
      "Cluster 7: health israel care insurance cost coverage problems universal need right\n",
      "Cluster 8: car seats cars vehicle repair great sharp drive honda body\n",
      "Cluster 9: game games metroid mario play mcgriff played ps3 going console\n",
      "Cluster 10: script words approach holes wise shit ex hair race despise\n",
      "Cluster 11: american toxic reservation government global liberals china warming google marine\n",
      "Cluster 12: went afternoon got home did bed planet italian tricia beach\n",
      "Cluster 13: jean started nbc came friend exams mall chapter abt overtime\n",
      "Cluster 14: obama health president care house reform republicans tax administration democrats\n",
      "Cluster 15: nbsp work python blog like class time numpy worked need\n",
      "Cluster 16: creative design marvel autism deliverables equality don people action favors\n",
      "Cluster 17: film movie films movies really scenes director bran like story\n",
      "Cluster 18: dont abt person ur friend really know testimonial sad say\n",
      "Cluster 19: wii directx telltale hidef game api max xbox netflix phone\n",
      "\n",
      "Females:\n",
      "KMeans:\n",
      "Cluster 0: room studio faith hair ship buried relationship movie restaurant feel\n",
      "Cluster 1: new work like world use time change way people know\n",
      "Cluster 2: nbsp love like just shop little book really want today\n",
      "Cluster 3: things knife just time little like water warm sharon cold\n",
      "Cluster 4: hair didn like night just time day mom got went\n",
      "Cluster 5: island going time family new week year visit beach trip\n",
      "Cluster 6: foods food wine australia dog eat area wines 2009 principles\n",
      "Cluster 7: just know like time don people really ve think want\n",
      "Cluster 8: ur baby dat cake da happy like really love smile\n",
      "Cluster 9: really girl make don just good love like women man\n",
      "Cluster 10: ve ll just like time know little got don new\n",
      "Cluster 11: school lunch students lunches kids food recess milk schools like\n",
      "Cluster 12: drugs doctors drug death disease people patients companies chavez said\n",
      "Cluster 13: class day just time children today did park kid like\n",
      "Cluster 14: weight blog life women people fat loss things really like\n",
      "Cluster 15: writing german write english language book search letter process speaking\n",
      "Cluster 16: abt tat miss wat shez frnd fun gud jus da\n",
      "Cluster 17: love life time city bus people way skin day friend\n",
      "Cluster 18: zune hd touch screen software glued vehicle monitor sale digital\n",
      "Cluster 19: wish day time maybe mom just good women know say\n",
      "\n",
      "MiniBatch:\n",
      "Cluster 0: allow missing baby yoga pressure head awhile process going keyboard\n",
      "Cluster 1: music meanings asia listen frm really allison bout invented keys\n",
      "Cluster 2: husband 30am meetings notes alarm read warm galicia gorgeously todd\n",
      "Cluster 3: google radar search template blogger new maps api map change\n",
      "Cluster 4: attic man alan catholic folder things teaching prisoner remember mr\n",
      "Cluster 5: party hotels beach sessions malatapay hilton thailand president fortunate vp\n",
      "Cluster 6: blog ur life challenge da list post like cleaned dat\n",
      "Cluster 7: skin cells water stratum layer melanin feet oil substance cleansing\n",
      "Cluster 8: daughter mother cake god just mom flowers dad flour cupcakes\n",
      "Cluster 9: sahuayo zune change hd insurance ideas changed story coaching plot\n",
      "Cluster 10: ms women pettigrew major mrs question lose asks men tyler\n",
      "Cluster 11: dog guinea pet dogs cat jeeves easter lou product spot\n",
      "Cluster 12: time like just day love school new make way know\n",
      "Cluster 13: knife airport flight sharon fish got just night little woody\n",
      "Cluster 14: nbsp like just little today love shop want really jordan\n",
      "Cluster 15: health care services bart abortion legislation needs medicare insurance premiums\n",
      "Cluster 16: im rice pub captain rained arms cooler snow song kind\n",
      "Cluster 17: jus lik tat kno light wen testi na screens fr\n",
      "Cluster 18: just like know ve time don really people good life\n",
      "Cluster 19: geisha want albums ve dancing slightly car beauty service obsess\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 20\n",
    "\n",
    "kmminiM = MiniBatchKMeans(n_clusters=n_clusters, init='k-means++', n_init=1,\n",
    "                     init_size=1000, batch_size=1000, verbose=False)\n",
    "kmminiF = MiniBatchKMeans(n_clusters=n_clusters, init='k-means++', n_init=1,\n",
    "                     init_size=1000, batch_size=1000, verbose=False)\n",
    "\n",
    "kmM = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
    "kmF = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
    "\n",
    "t0 = time()\n",
    "kmM.fit(XM)\n",
    "kmF.fit(XF)\n",
    "\n",
    "kmminiM.fit(XM)\n",
    "kmminiF.fit(XF)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "order_centroids_M = kmM.cluster_centers_.argsort()[:, ::-1]\n",
    "order_centroids_mini_M = kmminiM.cluster_centers_.argsort()[:, ::-1]\n",
    "order_centroids_F = kmF.cluster_centers_.argsort()[:, ::-1]\n",
    "order_centroids_mini_F = kmminiF.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "termsM = vectorizerM.get_feature_names()\n",
    "termsF = vectorizerF.get_feature_names()\n",
    "\n",
    "print(\"\\nMales:\")\n",
    "print(\"KMeans:\")\n",
    "for i in range(n_clusters):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids_M[i, :10]:\n",
    "        print(' %s' % termsM[ind], end='')\n",
    "    print()\n",
    "print()\n",
    "print(\"MiniBatch:\")\n",
    "for i in range(n_clusters):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids_mini_M[i, :10]:\n",
    "        print(' %s' % termsM[ind], end='')\n",
    "    print()\n",
    "\n",
    "print(\"\\nFemales:\")\n",
    "print(\"KMeans:\")\n",
    "for i in range(n_clusters):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids_F[i, :10]:\n",
    "        print(' %s' % termsF[ind], end='')\n",
    "    print()\n",
    "print()\n",
    "print(\"MiniBatch:\")\n",
    "for i in range(n_clusters):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids_mini_F[i, :10]:\n",
    "        print(' %s' % termsF[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
